0.解决GPU与CPU设备数据同步的问题

1.读取JSON数据，根据时间step，载入Env中的State
1.1 Env需要记住上一次的动作（reset如何处理上一个动作）---随机选择一个/上一个动作0
1.2 已完成的连接时间每次step是否需要重新计算（task里面的done_time???）
1.3 现在的卫星编号难以应用独热编码(先用卫星的数字id代替,但是由此导致动作空间的问题)----统计卫星的总共的数量

2.根据Paper设置Env中的reward计算（目前来看所有agent共同完成一个优化目标，因此reward相同，为优化目标的倒数）

3.训练模型，记录reward变化，保存模型，尝试调整输入数据的大小

！！！！！！
每个step都有25个新任务到来，
是不是应该重新设计为上一个step未完成的任务修改累计时间等参数，已完成的任务被替换为新的任务，
这样的模型更准确？？


done_time 指的是已经完成的连接时间
task.json 排队，time不一定等同；；；
reader再改一下


目前的设计：一共就25个task，每次step更新done_time，根据pos_id读取对应的卫星。已经完成的任务，done=1，体现在网络更新上。

1.还有一个问题，返回的action，此时task未必可见，（直接返回数值，在可见卫星中选择最大值）----->>>为什么有非独热编码？？？？？？
2.reward更新
3.take_action函数不直接返回独热编码，对应1.


12.1
1.目前的设计是，一共25个任务，执行完后agent不在进行其他任务；
2.done掉的任务，是否需要继续计算reward？？？
3.action的生成问题，Actor网络输出的动作未必可以被连接；（目前加了检测，Actor返回的是数值，
在Env中处理，无法连接就是全0，可以连接就改为独热编码）
4.Actor和Critic更新问题，done掉的任务，是否需要继续更新？？？


12.2
1.对于已经完成的任务，在计算reward时，一定会出现reward越来越小（因为已经完成的任务不参与reward计算，因此可以考虑使用平均数之类的操作）
2.关于动作，在计算critic时需要target_actor网络计算动作，如果修改了动作生成函数（改为非独热），会破坏MADDPG的逻辑；
但是如果不修改，Env中的选择便可能无法进行；（不修改的话，若选到不能连接的，直接取0）；
另外已经done掉的task是否应该继续生成动作？？？（我认为应该继续生成）；

12.4:
1.Actor网络的输出维度为3，按照顺序选择agent可见的卫星进行连接；
2.待调度的任务加入agent（具体体现为state值，但是已经执行完成的任务如何处理，全0还是暂时不动呢）；
3.需要观察的是return，还是每个时隙的reward；整体的reward大即可，还是看每个时隙的变化；
4.根据对reward的观察，r1，r3可能出现0，此时取倒数会导致reward极大；
5.另外，为什么在时隙较大时，还是有r1之类的，按理说此时大多数任务都完成了啊；
6.旧的任务完成了，加入新的任务；

12.10:
1.关于step的问题，S0与S0' 还是 S0与S1；
2.计算reward的取值问题，调参；

12.11：
1.S0输入根据自身得到的A0（step函数），立刻执行A0，更新state（k，done_time等），计算reward；查询t=1时刻的拓扑连接，更新state，得到S1；